# æ–‡ä»¶è·¯å¾„: web/crawler_hk.py
import akshare as ak
import pandas as pd
import time
import random
import math
import asyncio
import functools
import aiohttp
from pymongo import UpdateOne
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from database import stock_collection
from crawler_state import status
from config import NUMERIC_FIELDS
from logger import crawl_logger as logger

# === å…¨å±€å¹¶å‘é…ç½® ===
EXECUTOR = ThreadPoolExecutor(max_workers=5)

async def async_ak_call(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    pfunc = functools.partial(func, *args, **kwargs)
    return await loop.run_in_executor(EXECUTOR, pfunc)

async def async_db_call(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    pfunc = functools.partial(func, *args, **kwargs)
    return await loop.run_in_executor(EXECUTOR, pfunc)

def check_critical_error(e):
    err_str = str(e)
    if "Remote end closed connection" in err_str or "Connection aborted" in err_str or "RemoteDisconnected" in err_str:
        logger.critical(f"ğŸ›‘ ä¸¥é‡é”™è¯¯æ£€æµ‹: {err_str}")
        status.message = "âŒ è­¦å‘Šï¼šIPå¯èƒ½è¢«å°æˆ–è¿æ¥ä¸­æ–­ï¼Œä»»åŠ¡å¼ºåˆ¶ç»ˆæ­¢ï¼"
        status.should_stop = True 
        return True
    return False

def is_derivative(name):
    if not name: return False
    keywords = ['è´­', 'æ²½', 'ç‰›', 'ç†Š', 'ç•Œå†…', 'è³¼']
    for kw in keywords:
        if kw in name: return True
    return False

def get_ggt_codes():
    logger.info("ğŸ“¡ æ­£åœ¨è·å–æ¸¯è‚¡é€šæˆåˆ†è‚¡åå•...")
    try:
        df = ak.stock_hk_ggt_components_em()
        if df is not None and not df.empty:
            codes = df['ä»£ç '].astype(str).tolist()
            logger.info(f"âœ… è·å–åˆ° {len(codes)} åªæ¸¯è‚¡é€šè‚¡ç¥¨")
            return set(codes)
    except Exception as e:
        logger.warning(f"âš ï¸ æ¥å£è·å–æ¸¯è‚¡é€šåå•å¤±è´¥: {e} (å·²å¿½ç•¥é”™è¯¯ï¼Œå°è¯•åŠ è½½å†å²æ•°æ®...)")
    
    logger.info("âš ï¸ å°è¯•ä»æ•°æ®åº“åŠ è½½ã€å†å²æ¸¯è‚¡é€šæ•°æ®ã€‘...")
    try:
        cursor = stock_collection.find({"is_ggt": True}, {"_id": 1})
        codes = [doc["_id"] for doc in cursor]
        if codes: return set(codes)
    except Exception as db_e:
        logger.error(f"âŒ è¯»å–æ•°æ®åº“å¤±è´¥: {db_e}")
    return None 

def get_hk_codes_from_sina():
    logger.info("ğŸ“¡ è¿æ¥æ¥å£è·å–å…¨å¸‚åœºæ¸…å•...")
    try:
        df = ak.stock_hk_spot()
        if df is None or df.empty: return {}
        codes = df['ä»£ç '].astype(str).tolist()
        names = df['ä¸­æ–‡åç§°'].tolist()
        return dict(zip(codes, names))
    except Exception as e:
        check_critical_error(e)
        logger.error(f"âŒ è·å–åˆ—è¡¨å¤±è´¥: {e}")
        return {}

def compute_market_performance(df, h_share_capital=None):
    performance = {}
    if df is None or df.empty: return performance

    try:
        df = df.sort_values(by="date")
        if len(df) > 45: df = df.iloc[-45:]

        latest_row = df.iloc[-1]
        close_val = float(latest_row["close"])
        open_val = float(latest_row["open"])
        volume_val = float(latest_row["volume"])
        
        performance["æ˜¨æ”¶"] = close_val
        performance["æ˜¨æˆäº¤é‡"] = volume_val
        
        turnover_rate = 0.0
        if h_share_capital and h_share_capital > 0:
            try: turnover_rate = (volume_val / h_share_capital) * 100
            except: turnover_rate = 0.0
        
        performance["æ˜¨æ¢æ‰‹ç‡"] = round(turnover_rate, 2)

        if len(df) >= 2:
            prev_close = float(df.iloc[-2]["close"])
            if prev_close > 0:
                pct = (close_val - prev_close) / prev_close * 100
                performance["æ˜¨æ¶¨è·Œå¹…"] = round(pct, 2)
            else:
                performance["æ˜¨æ¶¨è·Œå¹…"] = 0.0
        else:
            if open_val > 0:
                pct = (close_val - open_val) / open_val * 100
                performance["æ˜¨æ¶¨è·Œå¹…"] = round(pct, 2)
            else:
                performance["æ˜¨æ¶¨è·Œå¹…"] = 0.0
        
        total_rows = len(df)
        if total_rows >= 6:
            prev_week_close = float(df.iloc[-6]["close"])
            if prev_week_close > 0:
                pct = (close_val - prev_week_close) / prev_week_close * 100
                performance["è¿‘ä¸€å‘¨æ¶¨è·Œå¹…"] = round(pct, 2)
        
        if total_rows >= 21:
            prev_month_close = float(df.iloc[-21]["close"])
            if prev_month_close > 0:
                pct = (close_val - prev_month_close) / prev_month_close * 100
                performance["è¿‘ä¸€æœˆæ¶¨è·Œå¹…"] = round(pct, 2)

    except Exception as e:
        logger.warning(f"âš ï¸ è®¡ç®—è¡Œæƒ…æŒ‡æ ‡å‡ºé”™: {e}")
        pass
    return performance

async def fetch_single_stock_op_async(code, name, is_ggt=None):
    if status.should_stop: return None
    if is_derivative(name): return None

    try:
        # === 1. å®šä¹‰å¹¶å‘ä»»åŠ¡ç»„ ===

        # ä»»åŠ¡A: ä¸œè´¢æ•°æ®ç»„ (åŒ…å«è´¢åŠ¡ã€æˆé•¿æ€§ã€å…¬å¸ç®€ä»‹)
        async def fetch_em_group():
            try:
                df_fin = await async_ak_call(ak.stock_hk_financial_indicator_em, symbol=code)
                await asyncio.sleep(0.3)
                df_growth = None
                try: df_growth = await async_ak_call(ak.stock_hk_growth_comparison_em, symbol=code)
                except: pass
                await asyncio.sleep(0.3)
                df_profile = None
                try: df_profile = await async_ak_call(ak.stock_hk_company_profile_em, symbol=code)
                except: pass
                return df_fin, df_growth, df_profile
            except Exception as e:
                if check_critical_error(e): raise e
                logger.warning(f"[{code}] è·å–è´¢åŠ¡æ•°æ®å¤±è´¥: {str(e)[:100]}")
                return None, None, None

        # ä»»åŠ¡B: é›ªçƒæ•°æ® (ç®€ä»‹)
        async def fetch_xq_intro():
            try:
                df_info = await async_ak_call(ak.stock_individual_basic_info_hk_xq, symbol=code)
                if df_info is not None and not df_info.empty:
                    mask = df_info['item'] == 'comintr'
                    if not mask.empty and mask.any():
                        return str(df_info.loc[mask, 'value'].iloc[0])
            except: pass
            return ""

        # ä»»åŠ¡C: è¡Œæƒ…æ•°æ® (æ—¥çº¿-ä¸å¤æƒ) - ç”¨äºè®¡ç®—æ˜¨æ—¥æ¶¨è·Œ
        async def fetch_market_daily():
            try:
                # ä¸å¤æƒï¼Œåæ˜ çœŸå®ä»·æ ¼
                df = await async_ak_call(ak.stock_hk_daily, symbol=code, adjust="")
                return df
            except Exception as e:
                if check_critical_error(e): raise e
                return None

        # [æ–°å¢] ä»»åŠ¡D: å†å²æ•°æ® (QFQ-å‰å¤æƒ) - ç”¨äºåç»­é•¿ç‰›å›æµ‹
        # é¢„åŠ è½½5å¹´ä»¥ä¸Šæ•°æ®ï¼Œä¸€åŠ³æ°¸é€¸
        async def fetch_qfq_history():
            try:
                df = await async_ak_call(
                    ak.stock_hk_hist, 
                    symbol=code, 
                    period="daily", 
                    start_date="20180101", 
                    end_date="22220101", 
                    adjust="qfq"
                )
                return df
            except Exception as e:
                # å†å²æ•°æ®å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                logger.warning(f"[{code}] è·å–QFQå†å²å¤±è´¥: {e}")
                return None

        # === 2. å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚ ===
        task_em = asyncio.create_task(fetch_em_group())
        task_xq = asyncio.create_task(fetch_xq_intro())
        task_market = asyncio.create_task(fetch_market_daily())
        task_qfq = asyncio.create_task(fetch_qfq_history())

        (df, df_growth_raw, df_profile_raw), intro_val, df_market_raw, df_qfq_raw = await asyncio.gather(
            task_em, task_xq, task_market, task_qfq
        )

        if df is None or df.empty: return None

        # === 3. æ•°æ®å¤„ç† (åŒæ­¥) ===
        date_col = None
        for col in ['æ—¥æœŸ', 'date', 'Date', 'ç»Ÿè®¡æ—¥æœŸ']:
            if col in df.columns:
                date_col = col
                break
        
        if date_col is None:
            today = datetime.now().strftime("%Y-%m-%d")
            df['æ—¥æœŸ'] = today
            date_col = 'æ—¥æœŸ'
            if len(df) > 1: df = df.iloc[[-1]]

        df[date_col] = pd.to_datetime(df[date_col]).dt.strftime("%Y-%m-%d")
        df.rename(columns={date_col: 'date'}, inplace=True)
        df = df.sort_values(by='date')

        h_share_capital = 0.0
        try:
            if not df.empty:
                last_row = df.iloc[-1]
                if "å·²å‘è¡Œè‚¡æœ¬-Hè‚¡(è‚¡)" in last_row:
                    val = last_row["å·²å‘è¡Œè‚¡æœ¬-Hè‚¡(è‚¡)"]
                    if pd.notna(val): h_share_capital = float(str(val).replace(',', ''))
        except: h_share_capital = 0.0

        growth_data = {}
        if df_growth_raw is not None and not df_growth_raw.empty:
            try:
                row_growth = df_growth_raw.iloc[0]
                target_keys = ["åŸºæœ¬æ¯è‚¡æ”¶ç›ŠåŒæ¯”å¢é•¿ç‡", "è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡", "è¥ä¸šåˆ©æ¶¦ç‡åŒæ¯”å¢é•¿ç‡"]
                for key in target_keys:
                    if key in df_growth_raw.columns:
                        val = row_growth[key]
                        if pd.notna(val) and val != "":
                            try: growth_data[key] = float(str(val).replace(',', ''))
                            except: growth_data[key] = val
            except: pass

        industry_val = ""
        if df_profile_raw is not None and not df_profile_raw.empty:
            if "æ‰€å±è¡Œä¸š" in df_profile_raw.columns:
                industry_val = str(df_profile_raw["æ‰€å±è¡Œä¸š"].iloc[0])

        market_data = compute_market_performance(df_market_raw, h_share_capital=h_share_capital)
        if status.should_stop: return None

        # [æ–°å¢] å¤„ç† QFQ å†å²æ•°æ®
        qfq_records = []
        if df_qfq_raw is not None and not df_qfq_raw.empty:
            try:
                # ç»Ÿä¸€åˆ—å
                rename_map = {
                    "æ—¥æœŸ": "date", "æ”¶ç›˜": "close", "å¼€ç›˜": "open", 
                    "æœ€é«˜": "high", "æœ€ä½": "low", "æˆäº¤é‡": "volume"
                }
                df_qfq_raw.rename(columns=rename_map, inplace=True)
                df_qfq_raw['date'] = pd.to_datetime(df_qfq_raw['date']).dt.strftime("%Y-%m-%d")
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                qfq_records = df_qfq_raw.to_dict('records')
            except Exception as e:
                logger.warning(f"[{code}] QFQå†å²æ•°æ®æ¸…æ´—å¤±è´¥: {e}")

        # === 4. æ„å»ºæ•°æ®åº“æ“ä½œ ===
        def prepare_db_op():
            existing_doc = stock_collection.find_one({"_id": code})
            history_map = {item["date"]: item for item in existing_doc.get("history", [])} if existing_doc else {}

            final_is_ggt = is_ggt if is_ggt is not None else existing_doc.get("is_ggt", False) if existing_doc else False
            latest_record = {}
            
            for _, row in df.iterrows():
                row_date = row['date']
                raw_data = row.to_dict()
                new_data = {}
                
                for k, v in raw_data.items():
                    if pd.isna(v): continue
                    should_convert = (k in NUMERIC_FIELDS)
                    clean_val = v
                    if should_convert:
                        try: clean_val = float(str(v).replace(',', ''))
                        except: clean_val = v
                    else:
                        if isinstance(v, str):
                             try:
                                 if "-" not in v and ":" not in v: 
                                     clean_val = float(v.replace(',', ''))
                             except: pass
                    new_data[k] = clean_val
                
                if industry_val: new_data['æ‰€å±è¡Œä¸š'] = industry_val
                if intro_val: new_data['ä¼ä¸šç®€ä»‹'] = intro_val
                new_data["date"] = row_date

                # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
                def get_v(keys):
                    for k in keys:
                        if k in new_data and isinstance(new_data[k], (int, float)): return new_data[k]
                    return None

                pe, eps, growth = get_v(['å¸‚ç›ˆç‡','PE']), get_v(['åŸºæœ¬æ¯è‚¡æ”¶ç›Š(å…ƒ)','åŸºæœ¬æ¯è‚¡æ”¶ç›Š']), get_v(['å‡€åˆ©æ¶¦æ»šåŠ¨ç¯æ¯”å¢é•¿(%)','å‡€åˆ©æ¶¦ç¯æ¯”å¢é•¿'])
                dividend_yield, ocf_ps = get_v(['è‚¡æ¯ç‡TTM(%)','è‚¡æ¯ç‡']), get_v(['æ¯è‚¡ç»è¥ç°é‡‘æµ(å…ƒ)','æ¯è‚¡ç»è¥ç°é‡‘æµ'])
                
                if "PEG" not in new_data and pe and pe > 0 and growth and growth != 0: new_data['PEG'] = round(pe / growth, 4)
                if pe and pe > 0 and growth is not None and dividend_yield is not None:
                    tr = growth + dividend_yield
                    if tr > 0: new_data['PEGY'] = round(pe / tr, 4)
                if growth is not None and eps is not None:
                    fp = eps * (8.5 + 2 * growth)
                    if fp > 0: new_data['åˆç†è‚¡ä»·'] = round(fp, 2)
                if ocf_ps and eps and eps > 0: new_data['å‡€ç°æ¯”'] = round(ocf_ps / eps, 2)

                if row_date in history_map: history_map[row_date].update(new_data)
                else: history_map[row_date] = new_data
                latest_record = history_map[row_date]

            if latest_record:
                if growth_data: latest_record.update(growth_data)
                if market_data: latest_record.update(market_data)
                if latest_record["date"] in history_map: history_map[latest_record["date"]].update(latest_record)

            sorted_history = sorted(history_map.values(), key=lambda x: x["date"])

            # [æ–°å¢] å°† QFQ å†å²æ•°æ®ä¹Ÿæ”¾å…¥ $set
            update_fields = {
                "name": name,
                "updated_at": datetime.now(),
                "latest_data": latest_record,
                "history": sorted_history,
                "industry": industry_val,
                "intro": intro_val,
                "is_ggt": final_is_ggt
            }
            if qfq_records:
                update_fields["qfq_history"] = qfq_records

            op = UpdateOne({"_id": code}, {"$set": update_fields}, upsert=True)
            return op

        op = await async_db_call(prepare_db_op)
        return op

    except aiohttp.ClientError as ne:
        logger.error(f"[{code}] ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {ne}")
    except asyncio.TimeoutError:
        logger.warning(f"[{code}] è¯·æ±‚è¶…æ—¶")
    except Exception as e:
        if check_critical_error(e): return None
        logger.error(f"[{code}] å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
        return None

def run_crawler_task():
    logger.info(f"[{datetime.now()}] ğŸš€ å¼€å§‹ MongoDB é‡‡é›†ä»»åŠ¡ (HK) - å¢å¼ºæ•°æ®ç‰ˆ...")
    
    logger.info("ğŸ§¹ æ­£åœ¨æ¸…ç† 8XXXX (äººæ°‘å¸æŸœå°) é‡å¤æ•°æ®...")
    stock_collection.delete_many({"_id": {"$regex": "^8"}})
    
    code_map = get_hk_codes_from_sina()
    if status.should_stop or not code_map: 
        status.finish("åˆå§‹åŒ–å¤±è´¥" if not code_map else status.message)
        return

    ggt_codes = get_ggt_codes()
    if ggt_codes:
        logger.info("âš¡ï¸ åˆ·æ–°å…¨åº“æ¸¯è‚¡é€šçŠ¶æ€...")
        try:
            l = list(ggt_codes)
            stock_collection.update_many({"_id": {"$in": l}}, {"$set": {"is_ggt": True}})
            stock_collection.update_many({"_id": {"$nin": l}}, {"$set": {"is_ggt": False}})
        except: pass

    all_codes = [(c, n) for c, n in code_map.items() if not c.startswith("8")]
    total = len(all_codes)
    logger.info(f"ğŸ“Š ä»»åŠ¡ç›®æ ‡: {total} åªè‚¡ç¥¨")
    
    status.start(total)
    BATCH_SIZE = 50

    async def main_crawl_loop():
        batch_ops = []
        for i, (code, name) in enumerate(all_codes):
            if status.should_stop:
                status.finish("ä»»åŠ¡ç»ˆæ­¢")
                return

            if code.startswith("043") and 4330 <= int(code) <= 4339:
                status.update(i + 1, message=f"è·³è¿‡(è¯•éªŒè®¡åˆ’): {name}")
                continue

            status.update(i + 1, message=f"æ­£åœ¨å¤„ç†: {name}")
            op = await fetch_single_stock_op_async(code, name, is_ggt=(code in ggt_codes if ggt_codes else None))
            
            if op: batch_ops.append(op)
            
            if len(batch_ops) >= BATCH_SIZE:
                try:
                    logger.info(f"âš¡ï¸ æäº¤ {len(batch_ops)} æ¡æ•°æ®...")
                    await async_db_call(stock_collection.bulk_write, batch_ops, ordered=False)
                    batch_ops = []
                except Exception as e:
                    logger.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                    batch_ops = []
            
            await asyncio.sleep(random.uniform(0.5, 1.5))
        
        if batch_ops:
            try:
                await async_db_call(stock_collection.bulk_write, batch_ops, ordered=False)
            except: pass

    try:
        asyncio.run(main_crawl_loop())
    except Exception as e:
        logger.error(f"âŒ å¾ªç¯å¼‚å¸¸: {e}")
        status.finish(f"å¼‚å¸¸: {e}")
        return
    
    status.finish("é‡‡é›†å®Œæˆ")
    logger.info(f"[{datetime.now()}] ğŸ‰ é‡‡é›†ä»»åŠ¡ç»“æŸ")