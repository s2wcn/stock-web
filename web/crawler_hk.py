# æ–‡ä»¶è·¯å¾„: web/crawler_hk.py
import akshare as ak
import pandas as pd
import asyncio
import functools
import aiohttp
import random
import time
from typing import Optional, List, Dict, Any, Tuple, Set
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from pymongo import UpdateOne
from database import stock_collection
from crawler_state import status
from config import NUMERIC_FIELDS, SystemConfig
from logger import crawl_logger as logger

# === çº¿ç¨‹æ± é…ç½® ===
# ä½¿ç”¨é…ç½®ä¸­çš„çº¿ç¨‹æ•°
EXECUTOR = ThreadPoolExecutor(max_workers=SystemConfig.CRAWLER_MAX_WORKERS)

async def async_ak_call(func, *args, **kwargs) -> Any:
    """é€šç”¨å¼‚æ­¥åŒ…è£…å™¨"""
    loop = asyncio.get_running_loop()
    pfunc = functools.partial(func, *args, **kwargs)
    return await loop.run_in_executor(EXECUTOR, pfunc)

async def async_db_call(func, *args, **kwargs) -> Any:
    """é€šç”¨æ•°æ®åº“å¼‚æ­¥åŒ…è£…å™¨"""
    loop = asyncio.get_running_loop()
    pfunc = functools.partial(func, *args, **kwargs)
    return await loop.run_in_executor(EXECUTOR, pfunc)

def get_ggt_codes() -> Optional[Set[str]]:
    """è·å–æ¸¯è‚¡é€šæ ‡çš„åˆ—è¡¨"""
    logger.info("ğŸ“¡ æ­£åœ¨è·å–æ¸¯è‚¡é€šæˆåˆ†è‚¡åå•...")
    try:
        df = ak.stock_hk_ggt_components_em()
        if df is not None and not df.empty:
            codes = df['ä»£ç '].astype(str).tolist()
            logger.info(f"âœ… è·å–åˆ° {len(codes)} åªæ¸¯è‚¡é€šè‚¡ç¥¨")
            return set(codes)
    except Exception as e:
        logger.warning(f"âš ï¸ æ¥å£è·å–æ¸¯è‚¡é€šåå•å¤±è´¥: {e} (å°è¯•ä»æ•°æ®åº“åŠ è½½)")
    
    try:
        cursor = stock_collection.find({"is_ggt": True}, {"_id": 1})
        codes = [doc["_id"] for doc in cursor]
        if codes: return set(codes)
    except Exception: pass
    return None

def get_hk_codes_from_sina() -> Dict[str, str]:
    """è·å–æ¸¯è‚¡å…¨å¸‚åœºä»£ç åˆ—è¡¨ (åŒæ­¥å‡½æ•°)"""
    df = ak.stock_hk_spot()
    if df is None or df.empty:
        raise ValueError("æ¥å£è¿”å›æ•°æ®ä¸ºç©º")
    codes = df['ä»£ç '].astype(str).tolist()
    names = df['ä¸­æ–‡åç§°'].tolist()
    return dict(zip(codes, names))

def check_data_freshness(threshold: float = 0.95) -> bool:
    """
    æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®æ˜¯å¦å·²ç»æ˜¯æœ€æ–°ã€‚
    """
    try:
        # 1. è·å–æ€»æ•° (æ’é™¤8å¼€å¤´)
        total_count = stock_collection.count_documents({"_id": {"$not": {"$regex": "^8"}}})
        if total_count == 0: return False

        # 2. æ‰¾åˆ°æœ€è¿‘çš„æ—¥æœŸ
        latest_doc = stock_collection.find_one(
            {"latest_data.date": {"$exists": True}}, 
            sort=[("latest_data.date", -1)]
        )
        if not latest_doc: return False
            
        max_date = latest_doc.get("latest_data", {}).get("date")
        if not max_date: return False

        # 3. ç»Ÿè®¡è¦†ç›–ç‡
        fresh_count = stock_collection.count_documents({
            "latest_data.date": max_date,
            "_id": {"$not": {"$regex": "^8"}}
        })

        ratio = fresh_count / total_count
        logger.info(f"ğŸ” æ•°æ®æ–°é²œåº¦æ£€æŸ¥: æœ€æ–°æ—¥æœŸ [{max_date}], è¦†ç›–ç‡ {fresh_count}/{total_count} ({ratio:.1%})")

        if ratio >= threshold:
            logger.info("âœ… æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡çˆ¬è™«é˜¶æ®µã€‚")
            return True
        else:
            logger.info("âš ï¸ æ•°æ®è¦†ç›–ç‡ä¸è¶³ï¼Œå‡†å¤‡å¯åŠ¨çˆ¬è™«...")
            return False

    except Exception as e:
        logger.error(f"âŒ æ–°é²œåº¦æ£€æŸ¥å¤±è´¥: {e}")
        return False

def compute_market_performance(df: pd.DataFrame, h_share_capital: float = 0.0) -> Dict[str, float]:
    """è®¡ç®—è¡Œæƒ…æŒ‡æ ‡"""
    performance = {}
    if df is None or df.empty: return performance

    try:
        df = df.sort_values(by="date")
        if len(df) > 45: df = df.iloc[-45:]

        latest_row = df.iloc[-1]
        close_val = float(latest_row["close"])
        open_val = float(latest_row["open"])
        volume_val = float(latest_row["volume"])
        
        performance["æ˜¨æ”¶"] = close_val
        performance["æ˜¨æˆäº¤é‡"] = volume_val
        
        turnover_rate = 0.0
        if h_share_capital > 0:
            try: turnover_rate = (volume_val / h_share_capital) * 100
            except: pass
        performance["æ˜¨æ¢æ‰‹ç‡"] = round(turnover_rate, 2)

        if len(df) >= 2:
            prev_close = float(df.iloc[-2]["close"])
            if prev_close > 0:
                pct = (close_val - prev_close) / prev_close * 100
                performance["æ˜¨æ¶¨è·Œå¹…"] = round(pct, 2)
            else:
                performance["æ˜¨æ¶¨è·Œå¹…"] = 0.0
        else:
            if open_val > 0:
                pct = (close_val - open_val) / open_val * 100
                performance["æ˜¨æ¶¨è·Œå¹…"] = round(pct, 2)

        total_rows = len(df)
        if total_rows >= 6:
            prev_week_close = float(df.iloc[-6]["close"])
            if prev_week_close > 0:
                pct = (close_val - prev_week_close) / prev_week_close * 100
                performance["è¿‘ä¸€å‘¨æ¶¨è·Œå¹…"] = round(pct, 2)
        
        if total_rows >= 21:
            prev_month_close = float(df.iloc[-21]["close"])
            if prev_month_close > 0:
                pct = (close_val - prev_month_close) / prev_month_close * 100
                performance["è¿‘ä¸€æœˆæ¶¨è·Œå¹…"] = round(pct, 2)

    except Exception as e:
        logger.warning(f"âš ï¸ è®¡ç®—è¡Œæƒ…æŒ‡æ ‡å‡ºé”™: {e}")
        pass
    return performance

async def fetch_single_stock_op_async(code: str, name: str, is_ggt: Optional[bool] = None) -> Optional[UpdateOne]:
    """æ ¸å¿ƒçˆ¬è™«é€»è¾‘"""
    if status.should_stop: return None

    try:
        # ä»»åŠ¡A: ä¸œè´¢æ•°æ®ç»„
        async def fetch_em_group():
            try:
                # ä½¿ç”¨ wait_for å¢åŠ å•æ¬¡è¯·æ±‚çš„è¶…æ—¶ä¿æŠ¤
                df_fin = await asyncio.wait_for(
                    async_ak_call(ak.stock_hk_financial_indicator_em, symbol=code), 
                    timeout=SystemConfig.API_TIMEOUT
                )
                await asyncio.sleep(SystemConfig.CRAWLER_REQUEST_DELAY)
                
                df_growth = None
                try: 
                    df_growth = await asyncio.wait_for(
                        async_ak_call(ak.stock_hk_growth_comparison_em, symbol=code),
                        timeout=SystemConfig.API_TIMEOUT
                    )
                except: pass
                
                df_profile = None
                try: 
                    df_profile = await asyncio.wait_for(
                        async_ak_call(ak.stock_hk_company_profile_em, symbol=code),
                        timeout=SystemConfig.API_TIMEOUT
                    )
                except: pass
                return df_fin, df_growth, df_profile
            except asyncio.TimeoutError:
                logger.warning(f"[{code}] è´¢åŠ¡æ•°æ®æ¥å£è¶…æ—¶")
                return None, None, None
            except Exception as e:
                logger.warning(f"[{code}] è·å–è´¢åŠ¡æ•°æ®å¤±è´¥: {str(e)[:100]}")
                return None, None, None

        # ä»»åŠ¡B: é›ªçƒç®€ä»‹
        async def fetch_xq_intro():
            try:
                df_info = await asyncio.wait_for(
                    async_ak_call(ak.stock_individual_basic_info_hk_xq, symbol=code),
                    timeout=10
                )
                if df_info is not None and not df_info.empty:
                    mask = df_info['item'] == 'comintr'
                    if not mask.empty and mask.any():
                        return str(df_info.loc[mask, 'value'].iloc[0])
            except: pass
            return ""

        # ä»»åŠ¡C: è¡Œæƒ…æ•°æ® (æ—¥çº¿)
        async def fetch_market_daily():
            try:
                return await asyncio.wait_for(
                    async_ak_call(ak.stock_hk_daily, symbol=code, adjust=""),
                    timeout=SystemConfig.API_TIMEOUT
                )
            except: return None

        # ä»»åŠ¡D: å†å²æ•°æ® (QFQ)
        async def fetch_qfq_history():
            try:
                return await asyncio.wait_for(
                    async_ak_call(
                        ak.stock_hk_hist, 
                        symbol=code, 
                        period="daily", 
                        start_date=SystemConfig.HISTORY_START_DATE, 
                        end_date=SystemConfig.HISTORY_END_DATE, 
                        adjust="qfq"
                    ),
                    timeout=SystemConfig.API_TIMEOUT
                )
            except: return None

        (df, df_growth_raw, df_profile_raw), intro_val, df_market_raw, df_qfq_raw = await asyncio.gather(
            fetch_em_group(), fetch_xq_intro(), fetch_market_daily(), fetch_qfq_history()
        )

        if df is None or df.empty: return None

        # === æ•°æ®æ¸…æ´— ===
        date_col = None
        for col in ['æ—¥æœŸ', 'date', 'Date', 'ç»Ÿè®¡æ—¥æœŸ']:
            if col in df.columns:
                date_col = col
                break
        
        if date_col is None:
            df['æ—¥æœŸ'] = datetime.now().strftime("%Y-%m-%d")
            date_col = 'æ—¥æœŸ'

        df[date_col] = pd.to_datetime(df[date_col]).dt.strftime("%Y-%m-%d")
        df.rename(columns={date_col: 'date'}, inplace=True)
        df = df.sort_values(by='date')

        h_share_capital = 0.0
        try:
            if not df.empty:
                last_row = df.iloc[-1]
                if "å·²å‘è¡Œè‚¡æœ¬-Hè‚¡(è‚¡)" in last_row:
                    val = last_row["å·²å‘è¡Œè‚¡æœ¬-Hè‚¡(è‚¡)"]
                    if pd.notna(val): h_share_capital = float(str(val).replace(',', ''))
        except: h_share_capital = 0.0

        growth_data = {}
        if df_growth_raw is not None and not df_growth_raw.empty:
            try:
                row_growth = df_growth_raw.iloc[0]
                target_keys = ["åŸºæœ¬æ¯è‚¡æ”¶ç›ŠåŒæ¯”å¢é•¿ç‡", "è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡", "è¥ä¸šåˆ©æ¶¦ç‡åŒæ¯”å¢é•¿ç‡"]
                for key in target_keys:
                    if key in df_growth_raw.columns:
                        val = row_growth[key]
                        if pd.notna(val) and val != "":
                            try: growth_data[key] = float(str(val).replace(',', ''))
                            except: growth_data[key] = val
            except: pass

        industry_val = ""
        if df_profile_raw is not None and not df_profile_raw.empty:
            if "æ‰€å±è¡Œä¸š" in df_profile_raw.columns:
                industry_val = str(df_profile_raw["æ‰€å±è¡Œä¸š"].iloc[0])

        market_data = compute_market_performance(df_market_raw, h_share_capital=h_share_capital)
        if status.should_stop: return None

        qfq_records = []
        if df_qfq_raw is not None and not df_qfq_raw.empty:
            try:
                rename_map = {"æ—¥æœŸ": "date", "æ”¶ç›˜": "close", "å¼€ç›˜": "open", "æœ€é«˜": "high", "æœ€ä½": "low", "æˆäº¤é‡": "volume"}
                df_qfq_raw.rename(columns=rename_map, inplace=True)
                df_qfq_raw['date'] = pd.to_datetime(df_qfq_raw['date']).dt.strftime("%Y-%m-%d")
                qfq_records = df_qfq_raw.to_dict('records')
            except Exception as e:
                logger.warning(f"[{code}] QFQå†å²æ•°æ®æ¸…æ´—å¤±è´¥: {e}")

        # === æ•°æ®åº“æ“ä½œæ„å»º ===
        def prepare_db_op():
            existing_doc = stock_collection.find_one({"_id": code})
            history_map = {item["date"]: item for item in existing_doc.get("history", [])} if existing_doc else {}
            final_is_ggt = is_ggt if is_ggt is not None else existing_doc.get("is_ggt", False) if existing_doc else False
            
            latest_record = {}
            for _, row in df.iterrows():
                row_date = row['date']
                new_data = row.to_dict()
                
                for k, v in new_data.items():
                    if pd.isna(v): continue
                    should_convert = (k in NUMERIC_FIELDS)
                    clean_val = v
                    if should_convert:
                        try: clean_val = float(str(v).replace(',', ''))
                        except: clean_val = v
                    else:
                        if isinstance(v, str) and "-" not in v and ":" not in v:
                             try: clean_val = float(v.replace(',', ''))
                             except: pass
                    new_data[k] = clean_val
                
                if industry_val: new_data['æ‰€å±è¡Œä¸š'] = industry_val
                if intro_val: new_data['ä¼ä¸šç®€ä»‹'] = intro_val
                new_data["date"] = row_date

                def get_v(keys):
                    for k in keys:
                        if k in new_data and isinstance(new_data[k], (int, float)): return new_data[k]
                    return None
                pe = get_v(['å¸‚ç›ˆç‡','PE'])
                eps = get_v(['åŸºæœ¬æ¯è‚¡æ”¶ç›Š(å…ƒ)','åŸºæœ¬æ¯è‚¡æ”¶ç›Š'])
                growth = get_v(['å‡€åˆ©æ¶¦æ»šåŠ¨ç¯æ¯”å¢é•¿(%)','å‡€åˆ©æ¶¦ç¯æ¯”å¢é•¿'])
                div_yield = get_v(['è‚¡æ¯ç‡TTM(%)','è‚¡æ¯ç‡'])
                ocf_ps = get_v(['æ¯è‚¡ç»è¥ç°é‡‘æµ(å…ƒ)','æ¯è‚¡ç»è¥ç°é‡‘æµ'])
                
                if "PEG" not in new_data and pe and pe > 0 and growth and growth != 0: 
                    new_data['PEG'] = round(pe / growth, 4)
                if pe and pe > 0 and growth is not None and div_yield is not None:
                    tr = growth + div_yield
                    if tr > 0: new_data['PEGY'] = round(pe / tr, 4)
                if growth is not None and eps is not None:
                    fp = eps * (8.5 + 2 * growth)
                    if fp > 0: new_data['åˆç†è‚¡ä»·'] = round(fp, 2)
                if ocf_ps and eps and eps > 0: 
                    new_data['å‡€ç°æ¯”'] = round(ocf_ps / eps, 2)

                if row_date in history_map: history_map[row_date].update(new_data)
                else: history_map[row_date] = new_data
                latest_record = history_map[row_date]

            if latest_record:
                if growth_data: latest_record.update(growth_data)
                if market_data: latest_record.update(market_data)
                if latest_record["date"] in history_map: history_map[latest_record["date"]].update(latest_record)

            sorted_history = sorted(history_map.values(), key=lambda x: x["date"])

            update_fields = {
                "name": name, "updated_at": datetime.now(), "latest_data": latest_record,
                "history": sorted_history, "industry": industry_val, "intro": intro_val, "is_ggt": final_is_ggt
            }
            if qfq_records: update_fields["qfq_history"] = qfq_records

            op = UpdateOne({"_id": code}, {"$set": update_fields}, upsert=True)
            return op

        op = await async_db_call(prepare_db_op)
        return op

    except Exception as e:
        logger.error(f"[{code}] å¤„ç†å¼‚å¸¸: {e}")
        return None

def run_crawler_task(force_update: bool = False):
    """çˆ¬è™«ä»»åŠ¡ä¸»å…¥å£"""
    # === [æ–°å¢] æ£€æŸ¥æ•°æ®æ˜¯å¦æœ€æ–° ===
    # å¦‚æœæ•°æ®åº“ä¸­ 95% ä»¥ä¸Šçš„æ•°æ®æ—¥æœŸéƒ½æ˜¯æœ€æ–°çš„ï¼Œä¸”ä¸å¼ºåˆ¶æ›´æ–°ï¼Œåˆ™è·³è¿‡çˆ¬è™«
    if not force_update:
        if check_data_freshness():
            return
    else:
        logger.info("ğŸ”¥ ç”¨æˆ·é€šè¿‡æŒ‡ä»¤å¼ºåˆ¶å¯åŠ¨çˆ¬è™« (å¿½ç•¥æ–°é²œåº¦æ£€æŸ¥)")

    logger.info(f"[{datetime.now()}] ğŸš€ å¼€å§‹ MongoDB é‡‡é›†ä»»åŠ¡ (HK) - ç¨³å¥ç‰ˆ...")
    stock_collection.delete_many({"_id": {"$regex": "^8"}})
    
    # === å¸¦è¶…æ—¶å’Œé‡è¯•çš„åˆ—è¡¨è·å– ===
    code_map = {}
    for attempt in range(SystemConfig.API_MAX_RETRIES):
        if status.should_stop: return
        try:
            logger.info(f"ğŸ“¡ è¿æ¥æ¥å£è·å–å…¨å¸‚åœºæ¸…å• (ç¬¬ {attempt+1} æ¬¡å°è¯•)...")
            
            # ä½¿ç”¨ asyncio.wait_for å¼ºåˆ¶è¶…æ—¶ç†”æ–­
            code_map = asyncio.run(asyncio.wait_for(
                async_ak_call(get_hk_codes_from_sina), 
                timeout=SystemConfig.API_TIMEOUT
            ))
            
            if code_map:
                logger.info(f"âœ… æˆåŠŸè·å– {len(code_map)} åªæ¸¯è‚¡")
                break
                
        except asyncio.TimeoutError:
            logger.warning(f"âš ï¸ æ¥å£å“åº”è¶…æ—¶ ({SystemConfig.API_TIMEOUT}s)ï¼Œç­‰å¾…é‡è¯•...")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¥å£æŠ¥é”™: {e}ï¼Œç­‰å¾…é‡è¯•...")
        
        time.sleep(3)
        
    if not code_map:
        status.finish("åˆå§‹åŒ–å¤±è´¥: æ— æ³•è¿æ¥è¡Œæƒ…æ¥å£")
        return

    ggt_codes = get_ggt_codes()
    if ggt_codes:
        logger.info("âš¡ï¸ åˆ·æ–°å…¨åº“æ¸¯è‚¡é€šçŠ¶æ€...")
        try:
            l = list(ggt_codes)
            stock_collection.update_many({"_id": {"$in": l}}, {"$set": {"is_ggt": True}})
            stock_collection.update_many({"_id": {"$nin": l}}, {"$set": {"is_ggt": False}})
        except: pass

    all_codes = [(c, n) for c, n in code_map.items() if not c.startswith("8")]
    status.start(len(all_codes))
    
    BATCH_SIZE = 50
    async def main_crawl_loop():
        batch_ops = []
        for i, (code, name) in enumerate(all_codes):
            if status.should_stop:
                status.finish("ä»»åŠ¡ç»ˆæ­¢")
                return

            if code.startswith("043") and 4330 <= int(code) <= 4339:
                status.update(i + 1, message=f"è·³è¿‡: {name}")
                continue

            status.update(i + 1, message=f"å¤„ç†: {name}")
            op = await fetch_single_stock_op_async(code, name, is_ggt=(code in ggt_codes if ggt_codes else None))
            
            if op: batch_ops.append(op)
            
            if len(batch_ops) >= BATCH_SIZE:
                try:
                    logger.info(f"âš¡ï¸ æäº¤ {len(batch_ops)} æ¡æ•°æ®...")
                    await async_db_call(stock_collection.bulk_write, batch_ops, ordered=False)
                    batch_ops = []
                except Exception as e:
                    logger.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                    batch_ops = []
            
            await asyncio.sleep(random.uniform(0.5, 1.5))
        
        if batch_ops:
            try: await async_db_call(stock_collection.bulk_write, batch_ops, ordered=False)
            except: pass

    try:
        asyncio.run(main_crawl_loop())
    except Exception as e:
        logger.error(f"âŒ å¾ªç¯å¼‚å¸¸: {e}")
        status.finish(f"å¼‚å¸¸: {e}")
        return
    
    status.finish("é‡‡é›†å®Œæˆ")
    logger.info(f"[{datetime.now()}] ğŸ‰ é‡‡é›†ä»»åŠ¡ç»“æŸ")